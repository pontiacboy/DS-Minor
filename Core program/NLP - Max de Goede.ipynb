{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook of Max de Goede on his NLP findings\n",
    "I have done some work with NLP in the past[1] [Click here to see this](https://github.com/pontiacboy/PDR_Supported/blob/master/NLP%20-%20Final%20-%20Max%20de%20Goede.ipynb), but I used techniques like Word2vec and Seq2Seq models. Now I will take the approach of looking at new techniques such as the usage of GPT2 and playing around with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "    <h1>Feedback Olaf</h1>\n",
    "    <br>\n",
    "Hi Max, thanks for the interesting read. I really like how you let it generate text and summarize itself and then apply sentiment analysis on both input and (summarized) text.\n",
    "    <h2> Secondary Mini-Application. Self reflecting Text Generator </h2>\n",
    "I would really like to see this in a combined mini-application (can just be a function within the notebook). Based on an input you could let the model generate 10 texts, which you summarize and sentiment-analyse. Based on comparing the score of where the sentiment conveys best the input, or where the summary is closest to the whole generated text, you return the best generated text. A self-reflecting text generator. :-)\n",
    "    <h2>Form</h2>\n",
    "On form: please hand in a pdf or upload your notebook to git so I can easily see a pretty-formatted output. Reread your text, you seem to contradict yourself: you say you will do transfer learning but you don't, you say you translate to Spanish/Dutch but then do it German etc.\n",
    "\n",
    "Otherwise nice work.\n",
    "    \n",
    " *Note, changes after feedback from Olaf will be noted as blue*\n",
    " </span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My idea\n",
    "There are 2<span style=\"color:blue\">(3)</span> things i would like to focus on:\n",
    "1. Pipelines\n",
    "- I would like to play with available pipelines from the Transformers library. To see what we can do with the various available networks. The main idea here is to get to know what these networks can do.\n",
    "2. Create a tool to converse with\n",
    "- I would like to create a small app that allows the user to have a conversation with a conversational NLP model. \n",
    "3. <span style=\"color:blue\"> Self reflecting Text Generator</span>\n",
    "- <span style=\"color:blue\"> I received feedback from Olaf, that it would be a cool idea to use all the techniques I explored during my research and combine them into 1. To generate an output that matches the input given. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Not many imports are necesarry, mainly the transformer library</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Conversation,AutoTokenizer, AutoModelForMaskedLM, BertConfig,pipeline, set_seed\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pipelines\n",
    "Let's play with some of the pipelines from the Transformers[2] library, with this we can get an idea of what we can do with the new techniques within NLP. We will look at the following things:\n",
    "1. Quick View\n",
    "2. Text generation\n",
    "3. Summarization\n",
    "4. Translate \n",
    "5. Sentiment analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Quickview\n",
    "We will be using GPT-2 and make a network that will be able to autofill something depending on what we type. as the input\n",
    "<br>\n",
    "<br>\n",
    "Let's start by importing the model. Let's take the Bert Model (this is just loading the config we don't do anything with this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = BertConfig.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"activation_function\": \"gelu_new\",\n",
       "  \"architectures\": [\n",
       "    \"GPT2LMHeadModel\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"attn_pdrop\": 0.1,\n",
       "  \"bos_token_id\": 50256,\n",
       "  \"embd_pdrop\": 0.1,\n",
       "  \"eos_token_id\": 50256,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"n_ctx\": 1024,\n",
       "  \"n_embd\": 768,\n",
       "  \"n_head\": 12,\n",
       "  \"n_layer\": 12,\n",
       "  \"n_positions\": 1024,\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"resid_pdrop\": 0.1,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"task_specific_params\": {\n",
       "    \"text-generation\": {\n",
       "      \"do_sample\": true,\n",
       "      \"max_length\": 50\n",
       "    }\n",
       "  },\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"vocab_size\": 50257\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets see how well the model performs when it tries to generate a story. Lets see what type of stories it comes up with! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2Model were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline ('text-generation',model = 'gpt2', tokenizer = tokenizer)\n",
    "set_seed(42)\n",
    "results = generator('Once upon a time',max_length=300, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'generated_text': 'Once upon a time, it was hard to have a relationship over beer. Beer, as you are talking now is the most popular drink on this planet and even the best of great ones have no effect on your physical well being (your immune system). At some point, a lot of the best and worst ones will come along and the result will be a mess and that\\'s always frustrating and even if, as it was, the beer was on the plate I\\'d definitely give up on it all.\\n\\nThe problem we are looking specifically at here is our inability to see an ounce of truth in the beer. There seems to be almost zero truth here. I mean, it\\'s a good beer, it might not last as long as some of the best but it\\'s not very good and, for me, the best I ever got is something named Bud Light. In between the two and maybe I\\'m not quite sure. Still, as a general rule, we feel like we all have a bit of a bad ass to have around.\\n\\nBeer doesn\\'t just work on your skin, it also works on your body and, in the case of Bud Light, it will do all sorts of things with what people refer to as \"your body\". Our skin is the layer of skin around everything and our brains are pretty much a part of that layer and it has these functions to them. Even within and around the human body, they all go hand in hand. If you think about it'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'generated_text': 'Once upon a time, those who are interested will have seen the great thing in them: One who has not yet reached the age of enlightenment, and without any knowledge of what he wishes (since the present life is such a journey) can attain true liberation, which means he can live for a lifetime. This is true in all spiritual fields. (2.) As time goes on, his true goal becomes clear, in this way he will be able to be known in all people in the world, since the one who seeks such an emancipation of life is the one who can live in the earth.\\n\\nIt is because of this, with the development of life and enlightenment, that one who has not yet reached the age of enlightenment, and without any knowledge of what he desires, can attain true liberation. He knows that this is necessary and can only be attained right away by all beings. (3.) This is one of those who are very fortunate in becoming able not only to speak out against ignorance on earth, but also to practice mindfulness in the same way, on the ground, on a high level, that one who has not yet reached the age of enlightenment, but without any knowledge of what he wishes (since the present life is such a journey) can attain true liberation. In other words the ultimate, final goal of the Buddha himself will be one who is able to be known fully through the whole life. And that is why a Buddha who has not yet reached it is called a'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'generated_text': 'Once upon a time, even when they were in the right place, the world was changing. And there were more people getting killed, and more dead in general. We didn\\'t know exactly how, but the people we were sending to stop our work were taking part in the events at that point, and we started a war of attrition between the nations of the region I was trying to take over.\\n\\nMy military adviser, Commander-in-Chief of the Joint Chiefs of Staff Thomas L. Bolton, was an enthusiastic supporter of a war plan. He said the war needed to continue, but there needed to be an \"international, regional and worldwide effort\" to ensure that all the members of America\\'s defense forces were ready for the war. A few years later, we reached an agreement to create our own National Security Council, to be headed by a military council composed of representatives from 50 countries. That was my idea, and that is what the United States is now.\\n\\nIn the second half of the twenty-first century, we are now seeing events in the Middle East, Africa, in developing nations becoming worse. I think it shows a serious lack of interest in the Middle East and Africa. I think that the American people will see the Middle East and Africa as bad, not as good, and that these bad things are part of a problem with the American economy and our national security, not an American one.\\n\\nThe most important thing is that we can make the world'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quickview Conclusion\n",
    "The results are pretty funny to look at and read. But of course this is not the type of proper usage we could use for the network. \n",
    "<br>\n",
    "<br> \n",
    "Up next we will look at what we can do with these networks. We will be using this page from huggingface(https://huggingface.co/transformers/main_classes/pipelines.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate text and then sumarizing it \n",
    "Lets, generate a text (about data science) and then lets see how well the text can be sumarized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Text generation\n",
    "Here we will give the pipeline the input of a start.  We will give it the following input \"Data Science is the future, especially when you look at what is possible with the NLP techniques\". Let's see what kind of results we get from this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "Story = generator('Data Science is the future, especially when you look at what is possible with the NLP techniques.',max_length=1000, num_return_sequences=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Data Science is the future, especially when you look at what is possible with the NLP techniques. While the NLP techniques have been used before by many companies, many companies have tried to leverage the NLP techniques into their products. However there are limitations and weaknesses to try and get the best results for your business.\\n\\nWe all know that your marketing budget is constantly changing due to business demand, customers demand, and so you might find your customers are looking for a high level of information. However, all these factors often add up and you may not be the marketer that they were hoping to be. The solution is to have information you have to fill in and then make better decisions on business changes.\\n\\n5. Be strategic about your SEO plans. SEO is changing all the time and it's not a new concept. SEO is very effective when it comes to identifying the best features for your pages, how your website looks and how to leverage those features to deliver the best result for the best customer experience for your business.\\n\\nAs a marketing manager my SEO strategy is to use data, strategies, and analysis that can help you to better target your client's interests and needs.\\n\\nAs I said before, we should put all our SEO resources in one place to build our content and to analyze what your customers are most eager to learn and think about.\\n\\n6. Create an organic email list for your business. Your business needs some SEO information and it's important to put that information right in front of your clients so they know exactly what they're looking for.\\n\\nLet's assume that you've got an organic email list, and you're a member of your newsletter list. But you're not the only one. You've probably put in a lot of work to keep your email list and newsletter list organized and optimized for your business.\\n\\nHere are a few tips you can use if you want to grow your business.\\n\\nAdd to it as little as possible. The big picture is that if your goal is organic content you need a good source of organic content. When your business is using organic content, it can become extremely difficult to get content on a daily basis.\\n\\nIn fact, many publishers have already started creating organic emails for them, so they might be able to grow the traffic they generate through all those practices.\\n\\nAs part of their efforts the best way to maximize the conversion rate for their emails is to combine things right. The best ways to do this are through a mix of using the NLP techniques, utilizing more specific data points, and using a combination of email and marketing tools to take advantage of the same opportunities they offer for a very competitive business.\\n\\nThe above list is a great beginning and long enough to be an effective and relevant guide for your business. If you have any questions about how to best optimize that list above, feel free to contact me and I'll take care of that for you.\\n\\nThere are many different ways to maximize your content including using direct marketing, SEO, and other free tools to get the most out of your email. I look forward to getting back at this as often as possible.\\n\\nHere are some of the most effective ways you should use your NLP on your email accounts:\\n\\nShare email to your friends and family. I know you have email accounts on social media like facebook and twitter as well. You don't need to do any such thing for your email account.\\n\\nUse social media instead of email. If you do send your email direct to your followers and then give them direct email contact, then they'll get that email much faster.\\n\\nUse a social media manager instead of a business email manager. In an email you need to have a social media manager, preferably one you can work with. A social media manager has no problem with a company email or corporate email and they will do everything they can to maximize your email.\\n\\nShare email to your competitors and bloggers. This is one of the best ways for business owners to optimize and reach their targets without feeling alone. It works best when both their customers and their customers aren't using email.\\n\\nUse Google for a great deal of your content. Some of what we discuss on SEO will be applicable to your business that you already know. But in my opinion most of it will be relevant to your existing business as well.\\n\\nHow can we make the best out of your personal email? We'd like to offer two tips to help you optimize your email.\\n\\n1. Know the best way email to reach you. For a personal email, be sure to set your priority and focus with your personal email only. However, you can also include everything you need in your email, so it always helps, even when it's just a quick email.\\n\\n1. Know the best way you can get from your product to your customers. You need to understand\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataScience_story = Story[0]['generated_text']\n",
    "DataScience_story"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like it was able to come up with quite a story on how you can use NLP in a business world and what type of effects it can have lets now try and sumarize the text!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Sumarization\n",
    "A few parameters are important in the pipeline to sumarize:\n",
    "1. Model\n",
    "2. Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5Model were not initialized from the model checkpoint at t5-base and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (998 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'data science is the future, especially when you look at what is possible with the NLP techniques . there are limitations and weaknesses to try and get the best results for your business . here are some of the most effective ways you should use your NLP on your email accounts . use direct marketing, SEO, and other free tools to get the most out of your email . know the best way email to reach you and how you can get from your product to your customers .'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = pipeline ('summarization',model = 't5-base', tokenizer='t5-base')\n",
    "Sumarization = generator(DataScience_story,min_length=100,max_length=300)\n",
    "Sumarization = Sumarization[0]['summary_text']\n",
    "Sumarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look of what the raw output would look like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Max_PC\\anaconda3\\envs\\clean_mask_rcnn\\lib\\site-packages\\transformers\\modeling_tf_auto.py:720: FutureWarning: The class `TFAutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `TFAutoModelForCausalLM` for causal language models, `TFAutoModelForMaskedLM` for masked language models and `TFAutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n",
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at t5-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[    0   331  2056    19     8   647     6   902   116    25   320    44\n",
      "    125    19   487    28     8   445  6892  2097     3     5   186   688\n",
      "     43  1971    12 11531     8   445  6892  2097   139    70   494     3\n",
      "      5   132    33 10005    11 21506    12   653    11   129     8   200\n",
      "    772    21    39   268     3     5]], shape=(1, 54), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelWithLMHead, AutoTokenizer\n",
    "\n",
    "model = TFAutoModelWithLMHead.from_pretrained(\"t5-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "\n",
    "inputs = tokenizer.encode(\"summarize: \" + DataScience_story, return_tensors=\"tf\", max_length=512)\n",
    "outputs = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Translate\n",
    "<span style=\"color:blue\">Initially I wanted to translate it to Dutch and Spanish, but it seems that these languages are not yet supported. It seems that Spanish and dutch are not yet supported and for this reason, we will be translating in a supported language german.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5Model were not initialized from the model checkpoint at t5-base and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'Datenwissenschaft ist die Zukunft, insbesondere wenn man sich anschaut, was mit den NLP-Techniken möglich ist . Es gibt Grenzen und Schwächen, um die besten Ergebnisse für Ihr Unternehmen zu erzielen . hier sind einige der effektivsten Möglichkeiten, wie Sie Ihre NLP auf Ihren E-Mail-Konten verwenden sollten . Nutzen Sie Direktmarketing, SEO und andere kostenlose Tools, um das Beste aus Ihrer E-Mail zu erhalten .'}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = pipeline ('translation_en_to_de', model = 't5-base')\n",
    "Translate = generator(Sumarization,max_length=300)\n",
    "Translate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at what the the output would look like, if we take this in an encoder/decoder basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at t5-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[    0 32099   331  2056    19     8   647     6   902   116    25   320\n",
      "     44   125    19   487    28     8   445  6892  2097     3     5   132\n",
      "     33 10005    11 21506    12   653    11   129     8   200   772    21\n",
      "     39   268     3     5]], shape=(1, 40), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelWithLMHead, AutoTokenizer\n",
    "\n",
    "model = TFAutoModelWithLMHead.from_pretrained(\"t5-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "\n",
    "inputs = tokenizer.encode(Sumarization, return_tensors=\"tf\")\n",
    "outputs = model.generate(inputs, max_length=40, num_beams=4, early_stopping=True)\n",
    "\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conlsusion\n",
    "The translation only worked on german, and with my vague understanding of german i think the text is fairly well trasnlated. Lets now try and check the sentiment of both the sumarized and full text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Sentiment analysis\n",
    "Lets check the sentiment of our generated text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'NEGATIVE', 'score': 0.8584626913070679}]\n",
      "[{'label': 'POSITIVE', 'score': 0.9984856247901917}]\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline ('sentiment-analysis')\n",
    "Sentimentsum = generator(Sumarization,max_length=300)\n",
    "print(Sentimentsum)\n",
    "Sentimentinput = generator('Data Science is the future, especially when you look at what is possible with the NLP techniques.',max_length=300)\n",
    "print(Sentimentinput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sadly the story as a whole could not be analized on sentiment as it was too long. But it was interesting to see that the sentiment of the summary was seen as negatiive. Even though the input text was nearly as positive as it could be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Converse tool \n",
    "Now let's go ahead and work on the project that we wanted to make, there are a few things that are important here:\n",
    "1. Creating the model that you can converse with.\n",
    "- This will be left on default which is a GPT2Model, I tried with both Bert & Rubert, but got results that were not satisfying at all. (I.e The language being returned was not recognizable)\n",
    "- Another part of this was finetuning the variables of the Conversation pipeline. It is important to know that 2 different variables need to be given to this model. One is the topic, so the model knows what it will be about. And the rest of the input will be inputs of the conversation. For the model, it is important to receive the whole conversation so it is aware of the conversation. \n",
    "2. The second part, is to create the loop to be able to converse with the model.\n",
    "- For this, it is important that you look at the input from the topic and the input of the messages separately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversational model\n",
    "We will create a conversational model with which the user can interact with. This leads to some pretty funny and interesting conversations. The user first has to input a topic and then the conversation can start. The network used is Gpt2 Conversational usage.\n",
    "<br>\n",
    "#### How to use the tool?\n",
    "1. Input a topic (the model is much better with certain topics than others). I got the feeling that it was good with small talk.\n",
    "2. After the topic input, one can give have the conversation. Talk about what you want and you will be able to see the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2Model were not initialized from the model checkpoint at microsoft/DialoGPT-medium and are newly initialized: ['transformer.h.0.attn.masked_bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.12.attn.masked_bias', 'transformer.h.13.attn.masked_bias', 'transformer.h.14.attn.masked_bias', 'transformer.h.15.attn.masked_bias', 'transformer.h.16.attn.masked_bias', 'transformer.h.17.attn.masked_bias', 'transformer.h.18.attn.masked_bias', 'transformer.h.19.attn.masked_bias', 'transformer.h.20.attn.masked_bias', 'transformer.h.21.attn.masked_bias', 'transformer.h.22.attn.masked_bias', 'transformer.h.23.attn.masked_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#DeepPavlov/bert-base-cased-conversational\n",
    "#DeepPavlov/rubert-base-cased-conversational\n",
    "#tokenizer = 't5-base'\n",
    "conversational_pipeline = pipeline(\"conversational\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose a topic:\n",
      "Hey, how are you?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation id: 3e85ea20-7a80-47f2-9ac5-b066224c2f55 \n",
      "user >> Hey, how are you? \n",
      "bot >> I'm good, how are you? \n",
      "\n",
      "About to start conversation\n",
      "I'm great thanks for asking\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation id: 3e85ea20-7a80-47f2-9ac5-b066224c2f55 \n",
      "user >> Hey, how are you? \n",
      "bot >> I'm good, how are you? \n",
      "user >> I'm great thanks for asking \n",
      "bot >> I'm glad to hear that. \n",
      "\n",
      "How old are you?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation id: 3e85ea20-7a80-47f2-9ac5-b066224c2f55 \n",
      "user >> Hey, how are you? \n",
      "bot >> I'm good, how are you? \n",
      "user >> I'm great thanks for asking \n",
      "bot >> I'm glad to hear that. \n",
      "user >> How old are you? \n",
      "bot >> I'm 19 \n",
      "\n",
      "And where are you from?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation id: 3e85ea20-7a80-47f2-9ac5-b066224c2f55 \n",
      "user >> Hey, how are you? \n",
      "bot >> I'm good, how are you? \n",
      "user >> I'm great thanks for asking \n",
      "bot >> I'm glad to hear that. \n",
      "user >> How old are you? \n",
      "bot >> I'm 19 \n",
      "user >> And where are you from? \n",
      "bot >> I'm from the UK \n",
      "\n",
      "Do you have a name?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation id: 3e85ea20-7a80-47f2-9ac5-b066224c2f55 \n",
      "user >> Hey, how are you? \n",
      "bot >> I'm good, how are you? \n",
      "user >> I'm great thanks for asking \n",
      "bot >> I'm glad to hear that. \n",
      "user >> How old are you? \n",
      "bot >> I'm 19 \n",
      "user >> And where are you from? \n",
      "bot >> I'm from the UK \n",
      "user >> Do you have a name? \n",
      "bot >> I don't, I'm not sure where I'm from \n",
      "\n",
      "Is it cold in the UK?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation id: 3e85ea20-7a80-47f2-9ac5-b066224c2f55 \n",
      "user >> Hey, how are you? \n",
      "bot >> I'm good, how are you? \n",
      "user >> I'm great thanks for asking \n",
      "bot >> I'm glad to hear that. \n",
      "user >> How old are you? \n",
      "bot >> I'm 19 \n",
      "user >> And where are you from? \n",
      "bot >> I'm from the UK \n",
      "user >> Do you have a name? \n",
      "bot >> I don't, I'm not sure where I'm from \n",
      "user >> Is it cold in the UK? \n",
      "bot >> It's cold in the UK \n",
      "\n",
      "Does it rain often?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation id: 3e85ea20-7a80-47f2-9ac5-b066224c2f55 \n",
      "user >> Hey, how are you? \n",
      "bot >> I'm good, how are you? \n",
      "user >> I'm great thanks for asking \n",
      "bot >> I'm glad to hear that. \n",
      "user >> How old are you? \n",
      "bot >> I'm 19 \n",
      "user >> And where are you from? \n",
      "bot >> I'm from the UK \n",
      "user >> Do you have a name? \n",
      "bot >> I don't, I'm not sure where I'm from \n",
      "user >> Is it cold in the UK? \n",
      "bot >> It's cold in the UK \n",
      "user >> Does it rain often? \n",
      "bot >> It's cold in the UK \n",
      "\n",
      "COld and rainy?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation id: 3e85ea20-7a80-47f2-9ac5-b066224c2f55 \n",
      "user >> Hey, how are you? \n",
      "bot >> I'm good, how are you? \n",
      "user >> I'm great thanks for asking \n",
      "bot >> I'm glad to hear that. \n",
      "user >> How old are you? \n",
      "bot >> I'm 19 \n",
      "user >> And where are you from? \n",
      "bot >> I'm from the UK \n",
      "user >> Do you have a name? \n",
      "bot >> I don't, I'm not sure where I'm from \n",
      "user >> Is it cold in the UK? \n",
      "bot >> It's cold in the UK \n",
      "user >> Does it rain often? \n",
      "bot >> It's cold in the UK \n",
      "user >> COld and rainy? \n",
      "bot >> It's cold in the UK \n",
      "\n",
      "It is nice talking with you\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation id: 3e85ea20-7a80-47f2-9ac5-b066224c2f55 \n",
      "user >> Hey, how are you? \n",
      "bot >> I'm good, how are you? \n",
      "user >> I'm great thanks for asking \n",
      "bot >> I'm glad to hear that. \n",
      "user >> How old are you? \n",
      "bot >> I'm 19 \n",
      "user >> And where are you from? \n",
      "bot >> I'm from the UK \n",
      "user >> Do you have a name? \n",
      "bot >> I don't, I'm not sure where I'm from \n",
      "user >> Is it cold in the UK? \n",
      "bot >> It's cold in the UK \n",
      "user >> Does it rain often? \n",
      "bot >> It's cold in the UK \n",
      "user >> COld and rainy? \n",
      "bot >> It's cold in the UK \n",
      "user >> It is nice talking with you \n",
      "bot >> I'm not a bad guy \n",
      "\n",
      "I know, bye\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation id: 3e85ea20-7a80-47f2-9ac5-b066224c2f55 \n",
      "user >> Hey, how are you? \n",
      "bot >> I'm good, how are you? \n",
      "user >> I'm great thanks for asking \n",
      "bot >> I'm glad to hear that. \n",
      "user >> How old are you? \n",
      "bot >> I'm 19 \n",
      "user >> And where are you from? \n",
      "bot >> I'm from the UK \n",
      "user >> Do you have a name? \n",
      "bot >> I don't, I'm not sure where I'm from \n",
      "user >> Is it cold in the UK? \n",
      "bot >> It's cold in the UK \n",
      "user >> Does it rain often? \n",
      "bot >> It's cold in the UK \n",
      "user >> COld and rainy? \n",
      "bot >> It's cold in the UK \n",
      "user >> It is nice talking with you \n",
      "bot >> I'm not a bad guy \n",
      "user >> I know, bye \n",
      "bot >>  \n",
      "\n",
      "stop\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Conversation id: 3e85ea20-7a80-47f2-9ac5-b066224c2f55 \n",
       "user >> Hey, how are you? \n",
       "bot >> I'm good, how are you? \n",
       "user >> I'm great thanks for asking \n",
       "bot >> I'm glad to hear that. \n",
       "user >> How old are you? \n",
       "bot >> I'm 19 \n",
       "user >> And where are you from? \n",
       "bot >> I'm from the UK \n",
       "user >> Do you have a name? \n",
       "bot >> I don't, I'm not sure where I'm from \n",
       "user >> Is it cold in the UK? \n",
       "bot >> It's cold in the UK \n",
       "user >> Does it rain often? \n",
       "bot >> It's cold in the UK \n",
       "user >> COld and rainy? \n",
       "bot >> It's cold in the UK \n",
       "user >> It is nice talking with you \n",
       "bot >> I'm not a bad guy \n",
       "user >> I know, bye \n",
       "bot >>  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Talkwith_model(message):\n",
    "    reply = conversational_pipeline(message,max_lenght = 300)\n",
    "    return reply\n",
    "\n",
    "def runtalk():\n",
    "    print('Choose a topic:')\n",
    "    topic = input()\n",
    "    topic = Conversation(topic)\n",
    "    print(conversational_pipeline(topic))\n",
    "    print('About to start conversation')\n",
    "    conv = True\n",
    "    listofmsg = []\n",
    "    while conv == True:\n",
    "        msg = input()\n",
    "        if msg =='stop':\n",
    "            conv = False\n",
    "        else:\n",
    "            topic.add_user_input(msg)\n",
    "            listofmsg.append(msg)\n",
    "            msg_to_send = Talkwith_model(topic)\n",
    "            print(msg_to_send)\n",
    "    return msg_to_send\n",
    "\n",
    "runtalk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose a topic:\n",
      "Do you have any movie suggestions?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation id: ad6c3c54-3b34-4007-b9c7-c3540b11b1ca \n",
      "user >> Do you have any movie suggestions? \n",
      "bot >> I'm not sure, but I'm sure there are some good ones. \n",
      "\n",
      "About to start conversation\n",
      "I like the star wars movies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation id: ad6c3c54-3b34-4007-b9c7-c3540b11b1ca \n",
      "user >> Do you have any movie suggestions? \n",
      "bot >> I'm not sure, but I'm sure there are some good ones. \n",
      "user >> I like the star wars movies \n",
      "bot >> I'm not sure if I can watch them, but I'll check them out. \n",
      "\n",
      "Why are you not sure if you can watch them?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation id: ad6c3c54-3b34-4007-b9c7-c3540b11b1ca \n",
      "user >> Do you have any movie suggestions? \n",
      "bot >> I'm not sure, but I'm sure there are some good ones. \n",
      "user >> I like the star wars movies \n",
      "bot >> I'm not sure if I can watch them, but I'll check them out. \n",
      "user >> Why are you not sure if you can watch them? \n",
      "bot >> I'm not sure if I can watch them, but I'll check them out. \n",
      "\n",
      "Cool, do you have a favorite movie?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation id: ad6c3c54-3b34-4007-b9c7-c3540b11b1ca \n",
      "user >> Do you have any movie suggestions? \n",
      "bot >> I'm not sure, but I'm sure there are some good ones. \n",
      "user >> I like the star wars movies \n",
      "bot >> I'm not sure if I can watch them, but I'll check them out. \n",
      "user >> Why are you not sure if you can watch them? \n",
      "bot >> I'm not sure if I can watch them, but I'll check them out. \n",
      "user >> Cool, do you have a favorite movie? \n",
      "bot >> I don't really have a favorite, but I like the ones that are good. \n",
      "\n",
      "What movies are good?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation id: ad6c3c54-3b34-4007-b9c7-c3540b11b1ca \n",
      "user >> Do you have any movie suggestions? \n",
      "bot >> I'm not sure, but I'm sure there are some good ones. \n",
      "user >> I like the star wars movies \n",
      "bot >> I'm not sure if I can watch them, but I'll check them out. \n",
      "user >> Why are you not sure if you can watch them? \n",
      "bot >> I'm not sure if I can watch them, but I'll check them out. \n",
      "user >> Cool, do you have a favorite movie? \n",
      "bot >> I don't really have a favorite, but I like the ones that are good. \n",
      "user >> What movies are good? \n",
      "bot >> I'm not sure, but I like the ones that are good. \n",
      "\n",
      "My name is max\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation id: ad6c3c54-3b34-4007-b9c7-c3540b11b1ca \n",
      "user >> Do you have any movie suggestions? \n",
      "bot >> I'm not sure, but I'm sure there are some good ones. \n",
      "user >> I like the star wars movies \n",
      "bot >> I'm not sure if I can watch them, but I'll check them out. \n",
      "user >> Why are you not sure if you can watch them? \n",
      "bot >> I'm not sure if I can watch them, but I'll check them out. \n",
      "user >> Cool, do you have a favorite movie? \n",
      "bot >> I don't really have a favorite, but I like the ones that are good. \n",
      "user >> What movies are good? \n",
      "bot >> I'm not sure, but I like the ones that are good. \n",
      "user >> My name is max \n",
      "bot >> I'm Max \n",
      "\n",
      "No i am max\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation id: ad6c3c54-3b34-4007-b9c7-c3540b11b1ca \n",
      "user >> Do you have any movie suggestions? \n",
      "bot >> I'm not sure, but I'm sure there are some good ones. \n",
      "user >> I like the star wars movies \n",
      "bot >> I'm not sure if I can watch them, but I'll check them out. \n",
      "user >> Why are you not sure if you can watch them? \n",
      "bot >> I'm not sure if I can watch them, but I'll check them out. \n",
      "user >> Cool, do you have a favorite movie? \n",
      "bot >> I don't really have a favorite, but I like the ones that are good. \n",
      "user >> What movies are good? \n",
      "bot >> I'm not sure, but I like the ones that are good. \n",
      "user >> My name is max \n",
      "bot >> I'm Max \n",
      "user >> No i am max \n",
      "bot >> Max \n",
      "\n",
      "What is your name?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation id: ad6c3c54-3b34-4007-b9c7-c3540b11b1ca \n",
      "user >> Do you have any movie suggestions? \n",
      "bot >> I'm not sure, but I'm sure there are some good ones. \n",
      "user >> I like the star wars movies \n",
      "bot >> I'm not sure if I can watch them, but I'll check them out. \n",
      "user >> Why are you not sure if you can watch them? \n",
      "bot >> I'm not sure if I can watch them, but I'll check them out. \n",
      "user >> Cool, do you have a favorite movie? \n",
      "bot >> I don't really have a favorite, but I like the ones that are good. \n",
      "user >> What movies are good? \n",
      "bot >> I'm not sure, but I like the ones that are good. \n",
      "user >> My name is max \n",
      "bot >> I'm Max \n",
      "user >> No i am max \n",
      "bot >> Max \n",
      "user >> What is your name? \n",
      "bot >> Max \n",
      "\n",
      "What is the weather like?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation id: ad6c3c54-3b34-4007-b9c7-c3540b11b1ca \n",
      "user >> Do you have any movie suggestions? \n",
      "bot >> I'm not sure, but I'm sure there are some good ones. \n",
      "user >> I like the star wars movies \n",
      "bot >> I'm not sure if I can watch them, but I'll check them out. \n",
      "user >> Why are you not sure if you can watch them? \n",
      "bot >> I'm not sure if I can watch them, but I'll check them out. \n",
      "user >> Cool, do you have a favorite movie? \n",
      "bot >> I don't really have a favorite, but I like the ones that are good. \n",
      "user >> What movies are good? \n",
      "bot >> I'm not sure, but I like the ones that are good. \n",
      "user >> My name is max \n",
      "bot >> I'm Max \n",
      "user >> No i am max \n",
      "bot >> Max \n",
      "user >> What is your name? \n",
      "bot >> Max \n",
      "user >> What is the weather like? \n",
      "bot >> Max \n",
      "\n",
      "stop\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Conversation id: ad6c3c54-3b34-4007-b9c7-c3540b11b1ca \n",
       "user >> Do you have any movie suggestions? \n",
       "bot >> I'm not sure, but I'm sure there are some good ones. \n",
       "user >> I like the star wars movies \n",
       "bot >> I'm not sure if I can watch them, but I'll check them out. \n",
       "user >> Why are you not sure if you can watch them? \n",
       "bot >> I'm not sure if I can watch them, but I'll check them out. \n",
       "user >> Cool, do you have a favorite movie? \n",
       "bot >> I don't really have a favorite, but I like the ones that are good. \n",
       "user >> What movies are good? \n",
       "bot >> I'm not sure, but I like the ones that are good. \n",
       "user >> My name is max \n",
       "bot >> I'm Max \n",
       "user >> No i am max \n",
       "bot >> Max \n",
       "user >> What is your name? \n",
       "bot >> Max \n",
       "user >> What is the weather like? \n",
       "bot >> Max "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runtalk()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">3. Self reflecting Text Generator</span>\n",
    "<span style=\"color:blue\">After recieving feedback from Olaf, he came with a good idea. To create a Self reflecting text generator. As we concluded that even though you give a positive input text, the generated text would come out as negative. But the model can generate multiple texts. So it would be a cool idea how this would turn out if we make a mini-application that could evaluate multiple texts and return the sumarization which most closely resembles the input text when it comes to sentiment.</span>\n",
    "### <span style=\"color:blue\"> Necessary actions to create mini-application</span> \n",
    "#### <span style=\"color:blue\">1. Generating text from input</span>\n",
    "- <span style=\"color:blue\">The model which initiates a text generation receives an input text, this can be as long as the user wants it to be. A text will then be generated form the input</span>\n",
    "- <span style=\"color:blue\">**For future improvements the user can think of creating parameters which alllow the user to change the lenght of the generated text.**</span>\n",
    "#### <span style=\"color:blue\">2. Sumarize text</span>\n",
    "- <span style=\"color:blue\">Sumarization of the text</span>\n",
    "- <span style=\"color:blue\">**A possible improvement would be to let the network generate multiple sumaries and then return the one which best represents the sentimental value**</span>\n",
    "#### <span style=\"color:blue\">3. Analysing summary sentiment</span>\n",
    "- <span style=\"color:blue\">Apply sentimental analysis to the summarization of the text.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2Model were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of T5Model were not initialized from the model checkpoint at t5-base and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1053 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input string sentiment: [{'label': 'POSITIVE', 'score': 0.9984856247901917}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 300, but you input_length is only 271. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story 1: [{'label': 'POSITIVE', 'score': 0.7254397869110107}]\n",
      "Story 2: [{'label': 'NEGATIVE', 'score': 0.9757229685783386}]\n",
      "Story 3: [{'label': 'NEGATIVE', 'score': 0.9975986480712891}]\n",
      "Story 4: [{'label': 'POSITIVE', 'score': 0.9992242455482483}]\n",
      "Story 5: [{'label': 'NEGATIVE', 'score': 0.999736487865448}]\n",
      "Story 6: [{'label': 'NEGATIVE', 'score': 0.9986565113067627}]\n",
      "Story 7: [{'label': 'POSITIVE', 'score': 0.9966144561767578}]\n",
      "Story 8: [{'label': 'NEGATIVE', 'score': 0.9887039065361023}]\n",
      "Story 9: [{'label': 'NEGATIVE', 'score': 0.970182478427887}]\n",
      "Story 10: [{'label': 'NEGATIVE', 'score': 0.9086717963218689}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The closest matching text on a sentimental basis. Sentimental rate input :0.9984856247901917 Sentimental rate output:0.9986565113067627------------------------------------------------------------------------------------------------------------------------------------- Generated summary: many frozen foods don't have a way to store all the ingredients until you first get them out of the bag . if you have lots of cheese on hand, use only 1 tbsp, or one whole egg per egg . you can also use other cheeses to your advantage, such as butter as a base for your bacon cheesecake . be sure to place all the cheese on the stove top, not the baking sheet or the oven .\""
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_textgeneration_sentiment(inputstring):\n",
    "    ########################################################################################\n",
    "    # Function inputs:                                                                     #\n",
    "    #                 - inputstring, start of the storyline                                #\n",
    "    ########################################################################################\n",
    "    \n",
    "    # Generate all pipeline variables    \n",
    "    txtgen = pipeline ('text-generation',model = 'gpt2', tokenizer = tokenizer)\n",
    "    sumagen = pipeline ('summarization',model = 't5-base', tokenizer='t5-base')\n",
    "    sentiggenerator = pipeline ('sentiment-analysis')\n",
    "    Story = txtgen(inputstring,max_length=1000, num_return_sequences=10)\n",
    "    i = 0\n",
    "    inputsent = sentiggenerator(inputtxt,max_length=300)\n",
    "    inputsentiment = inputsent[0]['score']\n",
    "    print('Input string sentiment: ' + str(inputsent))\n",
    "    \n",
    "    # Create array's to save the output from the pipelines\n",
    "    stories = []\n",
    "    textsentl = []\n",
    "    textsents = []\n",
    "    suma = []\n",
    "    \n",
    "    # Loop where the values get added ot the array\n",
    "    for items in Story:\n",
    "        text = Story[i]['generated_text']\n",
    "        i = i + 1\n",
    "        stories.append(text)\n",
    "        sumarization = sumagen(text,min_length=100,max_length=300)\n",
    "        suma.append(sumarization[0]['summary_text'])\n",
    "        textsentiment = sentiggenerator(sumarization[0]['summary_text'],max_length=300)\n",
    "        textsentl.append(textsentiment[0]['label'])\n",
    "        textsents.append(textsentiment[0]['score'])\n",
    "        print('Story ' + str(i)+ ': ' + str(textsentiment))\n",
    "    \n",
    "    # Matching the closest sentimental values. \n",
    "    sentiment_matching = min(textsents, key=lambda x:abs(x-inputsentiment))\n",
    "    indexwithmatch = textsents.index(sentiment_matching)\n",
    "    \n",
    "    # Gnerate reply\n",
    "    reply = 'The closest matching text on a sentimental basis. Sentimental rate input :'+str(inputsentiment)+ ' Sentimental rate output:' + str(textsents[indexwithmatch]) +    '------------------------------------------------------------------------------------------------------------------------------------- Generated summary: ' +suma[indexwithmatch]\n",
    "    return reply\n",
    "run_textgeneration_sentiment('Once upon a time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2Model were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of T5Model were not initialized from the model checkpoint at t5-base and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1020 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input string sentiment: [{'label': 'POSITIVE', 'score': 0.9984856247901917}]\n",
      "Story 1: [{'label': 'NEGATIVE', 'score': 0.8107666373252869}]\n",
      "Story 2: [{'label': 'NEGATIVE', 'score': 0.9921165704727173}]\n",
      "Story 3: [{'label': 'POSITIVE', 'score': 0.9733794331550598}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 300, but you input_length is only 159. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story 4: [{'label': 'NEGATIVE', 'score': 0.6406677961349487}]\n",
      "Story 5: [{'label': 'NEGATIVE', 'score': 0.7683529853820801}]\n",
      "Story 6: [{'label': 'POSITIVE', 'score': 0.809773325920105}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 300, but you input_length is only 91. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story 7: [{'label': 'POSITIVE', 'score': 0.8215650320053101}]\n",
      "Story 8: [{'label': 'NEGATIVE', 'score': 0.9646899700164795}]\n",
      "Story 9: [{'label': 'NEGATIVE', 'score': 0.9932735562324524}]\n",
      "Story 10: [{'label': 'POSITIVE', 'score': 0.9691058993339539}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The closest matching text on a sentimental basis. Sentimental rate input :0.9984856247901917 Sentimental rate output:0.9932735562324524------------------------------------------------------------------------------------------------------------------------------------- Generated summary: text processing is designed to give users a sense of the human language and the meaning of its words . many of these tools are not available in most countries such as the u.s. or the uk . codepredict is a tool used by several leading computer companies to create programs that can be run in a few seconds (often in seconds or minutes) on a standard machine . in this blog, we will show you how to use a free scripting tool such as CodePredict .'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_textgeneration_sentiment('Natural language processing is an interesting topic!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "My conclusion will contain, my point of view on a few insights. I will focus on what differences I see between networks such as GPT2 and the older work I did and I will focus on the findings that I found from working on this.\n",
    "##### 1. What has changed?\n",
    "Looking at what was possible in the past (last year), to be able to use these techniques you had to import multiple libraries to work with NLP. It is extremely handy that transformers have multiple NLP pipelines and that it supports all of the available networks on huggingface.co[huggingface.co]. It makes the usage much easier, **THOUGH** I will add that finding information on how to use the pipelines or retrain them was not an easy task but once it was easy to follow the documentation. I could not find information on how to retrain an existing network. Luckily this was not the task, and such I did my best playing around with the pipelines to see what you can do with the transformers library.\n",
    "##### 2. Pipeline and usage.\n",
    "The usage of the pipelines is extremely handy, especially with the fact that you can choose what model to run and add a tokenizer. The library is very well documented and as such is easy to use. Playing around with it taught me how this can be of use.<br><br> \n",
    "Already seeing the conversational model it is good for small talk. But once you start specifying the conversation you can tell that the model has trouble keeping up. Especially since it tries to keep in mind what the conversation is about.<br><br>\n",
    "<span style=\"color:blue\">The mini-application can self reflect on the text it generates and can choose the text which best matches the sentimental value of the input. Seeing how an application like this can be made is quite scary. And the scary thing is not that it is possible, but how easy it is to apply. I can already get an idea on how I could generate tweets given that I give it a set input and how I could filter (for example) the positive tweets out so the bot could proceed to only post negative tweets on a topic.</span>\n",
    "##### 3. The future\n",
    "Seeing what Olaf showed us, it seems very scary. As the NLP network could program HTLM, just by giving it inputs of what it should look like. Just by seeing what is available already, it is impressive. And how a network can generate a decent text just by giving it an input. The question is at what point can we tell the difference? This can be a huge topic in the coming years. I think NLP is one (IF THE MOST) dangerous part of Machine Learning. As the one thing that puts us above other species is the ability to communicate in the way that we do. If a computer can invade this part of us humans, we come to possibly the biggest ethical questions that we could ever face."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "[1] de Goede Max (2020). Github Repo. Retrived from: https://github.com/pontiacboy/PDR_Supported/blob/master/NLP%20-%20Final%20-%20Max%20de%20Goede.ipynb<br>\n",
    "[2] Wolf et al (2019). HuggingFace's Transformers: State-of-the-art Natural Language Processing. Huggingface.co. Retrieved from https://huggingface.co/transformers/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
